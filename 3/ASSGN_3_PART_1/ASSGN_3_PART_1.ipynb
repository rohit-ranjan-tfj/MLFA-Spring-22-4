{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "os1FU-P_P6kU"
   },
   "outputs": [],
   "source": [
    "# MLFA Assignment 3\n",
    "\n",
    "# Team Members - \n",
    "# Deepansh Agrawal - 19MI10018\n",
    "# Rohit Ranjan - 20CS30066\n",
    "# Neha Gupta - 20CH10094\n",
    "# Gautam Jaju - 20AG30015\n",
    "# Siddharth Madhupati - 20ME30083\n",
    "# Madiha Hanifa - 20MF10018\n",
    "# Himadri Pandya - 20ME10047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P9oFtWQfOw31"
   },
   "outputs": [],
   "source": [
    "# import commands\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# loading the dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wvlwM8dUS3mN"
   },
   "outputs": [],
   "source": [
    "# reshaping the input data to the range 0 - 1\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfXq-9aqS9-4",
    "outputId": "d9d30256-4d85-40e4-9df1-18d902cdfdab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cBmsTsr6Qr0r"
   },
   "outputs": [],
   "source": [
    "# Implementation of the uniform network\n",
    "# There are 88 neurons in each layer except the first layer and the output layer\n",
    "# The parameters of the input layer (39250) and first layer (4488) are discounted\n",
    "# The number of parameters is constant after that\n",
    "# The number of parameters is different in the last hidden layer as specified in the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iGiF43ig4aNT"
   },
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "UniformNet = keras.Sequential([\n",
    "                               keras.layers.InputLayer((784)),\n",
    "                               keras.layers.Dense(50, activation='relu', name='first_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='second_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='third_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='fourth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='fifth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='sixth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='seventh_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='eighth_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(88, activation='relu', name='nineth_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(10, activation='softmax', name='output_layer', kernel_initializer=initializer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceuB_sk1Coxn",
    "outputId": "746e106d-f08a-401a-cf21-930f694dc894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " first_layer (Dense)         (None, 50)                39250     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 50)               200       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " second_layer (Dense)        (None, 88)                4488      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " third_layer (Dense)         (None, 88)                7832      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " fourth_layer (Dense)        (None, 88)                7832      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 88)                0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " fifth_layer (Dense)         (None, 88)                7832      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 88)                0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " sixth_layer (Dense)         (None, 88)                7832      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88)                0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " seventh_layer (Dense)       (None, 88)                7832      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " eighth_layer (Dense)        (None, 88)                7832      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " nineth_layer (Dense)        (None, 88)                7832      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 88)               352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                890       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,468\n",
      "Trainable params: 100,960\n",
      "Non-trainable params: 1,508\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "UniformNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cMyVNGH8KRtn"
   },
   "outputs": [],
   "source": [
    "UniformNet.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCd5spgzKSD2",
    "outputId": "ed970f8c-97f4-466c-9df1-d8bdd7efa8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 10s 15ms/step - loss: 2.3375 - accuracy: 0.2103\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.6245 - accuracy: 0.4308\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.1427 - accuracy: 0.6094\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.8326 - accuracy: 0.7289\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.6374 - accuracy: 0.8036\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5170 - accuracy: 0.8464\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.4356 - accuracy: 0.8745\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3830 - accuracy: 0.8918\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3396 - accuracy: 0.9048\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3092 - accuracy: 0.9142\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2814 - accuracy: 0.9223\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2645 - accuracy: 0.9281\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2478 - accuracy: 0.9330\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2300 - accuracy: 0.9379\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2185 - accuracy: 0.9418\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2094 - accuracy: 0.9442\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1960 - accuracy: 0.9481\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1858 - accuracy: 0.9499\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1771 - accuracy: 0.9525\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1685 - accuracy: 0.9547\n",
      "79/79 [==============================] - 1s 3ms/step - loss: 0.1397 - accuracy: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1396992951631546, 0.9631999731063843]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniformNet.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1)\n",
    "UniformNet.evaluate(x_test, y_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rYNkutUrSl4J"
   },
   "outputs": [],
   "source": [
    "# Implementation of the pyramid network\n",
    "# There are varying number of neurons in each layer\n",
    "# The parameters of the input layer (39250) and first layer (200) are discounted\n",
    "# The number of parameters falls by a factor of two after that\n",
    "# The number of parameters is different in the last hidden layer as specified in the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MSRodqIEEwR4"
   },
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "PyramidNet = keras.Sequential([\n",
    "                               keras.layers.InputLayer((784)),\n",
    "                               keras.layers.Dense(50, activation='relu', name='first_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(16, activation='relu', name='second_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(1250, activation='relu', name='third_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(8, activation='relu', name='fourth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(625, activation='relu', name='fifth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(4, activation='relu', name='sixth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(312, activation='relu', name='seventh_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(2, activation='relu', name='eighth_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(156, activation='relu', name='nineth_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(10, activation='softmax', name='output_layer', kernel_initializer=initializer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7x_dvRYhG4I3",
    "outputId": "fde5bf5f-a6e5-40ee-d836-1e4e1bb00ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " first_layer (Dense)         (None, 50)                39250     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 50)               200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " second_layer (Dense)        (None, 16)                816       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " third_layer (Dense)         (None, 1250)              21250     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 1250)             5000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fourth_layer (Dense)        (None, 8)                 10008     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fifth_layer (Dense)         (None, 625)               5625      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 625)               0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 625)              2500      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " sixth_layer (Dense)         (None, 4)                 2504      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 4)                16        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " seventh_layer (Dense)       (None, 312)               1560      \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 312)              1248      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " eighth_layer (Dense)        (None, 2)                 626       \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 2)                8         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " nineth_layer (Dense)        (None, 156)               468       \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 156)              624       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                1570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,369\n",
      "Trainable params: 88,523\n",
      "Non-trainable params: 4,846\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PyramidNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7TM5nrQnI2ln"
   },
   "outputs": [],
   "source": [
    "PyramidNet.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBPCmepNI2od",
    "outputId": "d2070abd-87b7-4f12-b91b-31aa1a9e4c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 10s 16ms/step - loss: 2.2005 - accuracy: 0.1786\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 1.8831 - accuracy: 0.2492\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.6977 - accuracy: 0.3248\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 1.5016 - accuracy: 0.3735\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.4055 - accuracy: 0.4181\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.3172 - accuracy: 0.4679\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.2528 - accuracy: 0.5031\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.2146 - accuracy: 0.5209\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.1708 - accuracy: 0.5349\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 1.1302 - accuracy: 0.5505\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.0979 - accuracy: 0.5633\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.0546 - accuracy: 0.5766\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.0418 - accuracy: 0.5830\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 1.0223 - accuracy: 0.5849\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.9996 - accuracy: 0.5920\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.9871 - accuracy: 0.5903\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 0.9787 - accuracy: 0.5960\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 0.9618 - accuracy: 0.5973\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 0.9544 - accuracy: 0.6003\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 0.9478 - accuracy: 0.5988\n",
      "79/79 [==============================] - 1s 4ms/step - loss: 2.3110 - accuracy: 0.2793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.311028480529785, 0.2793000042438507]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyramidNet.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1)\n",
    "PyramidNet.evaluate(x_test, y_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "by_fntT6TYwL"
   },
   "outputs": [],
   "source": [
    "# Implementation of the pyramid network\n",
    "# There are varying number of neurons in each layer\n",
    "# The parameters of the input layer (39250) and first layer (255) are discounted\n",
    "# The number of parameters increases by a factor of two after that\n",
    "# The number of parameters is different in the last hidden layer as specified in the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TBUgNrCmI2q_"
   },
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "InvPyramidNet = keras.Sequential([\n",
    "                               keras.layers.InputLayer((784)),\n",
    "                               keras.layers.Dense(50, activation='relu', name='first_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(5, activation='relu', name='second_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(94, activation='relu', name='third_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(10, activation='relu', name='fourth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(188, activation='relu', name='fifth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(20, activation='relu', name='sixth_layer', kernel_initializer=initializer), Dropout(0.3), BatchNormalization(),\n",
    "                               keras.layers.Dense(376, activation='relu', name='seventh_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(40, activation='relu', name='eighth_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(750, activation='relu', name='nineth_layer', kernel_initializer=initializer), BatchNormalization(),\n",
    "                               keras.layers.Dense(10, activation='softmax', name='output_layer', kernel_initializer=initializer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXi3-mEXI2tg",
    "outputId": "04e17329-012e-47d7-82d5-5117edd2d16c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " first_layer (Dense)         (None, 50)                39250     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 50)               200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " second_layer (Dense)        (None, 5)                 255       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 5)                20        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " third_layer (Dense)         (None, 94)                564       \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 94)               376       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fourth_layer (Dense)        (None, 10)                950       \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 10)               40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " fifth_layer (Dense)         (None, 188)               2068      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 188)               0         \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 188)              752       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " sixth_layer (Dense)         (None, 20)                3780      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 20)               80        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " seventh_layer (Dense)       (None, 376)               7896      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 376)              1504      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " eighth_layer (Dense)        (None, 40)                15080     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 40)               160       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " nineth_layer (Dense)        (None, 750)               30750     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 750)              3000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                7510      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,235\n",
      "Trainable params: 111,169\n",
      "Non-trainable params: 3,066\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "InvPyramidNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hgm-Cm3oI2w-"
   },
   "outputs": [],
   "source": [
    "InvPyramidNet.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nx8gUKRqqtbL",
    "outputId": "1828c8f4-689d-45ae-bd8c-012d6f5f8b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 8s 13ms/step - loss: 2.1832 - accuracy: 0.2240\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 1.4959 - accuracy: 0.4511\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 1.1774 - accuracy: 0.5608\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 1.0142 - accuracy: 0.6153\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.9120 - accuracy: 0.6561\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.8488 - accuracy: 0.6820\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.7879 - accuracy: 0.6998\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.7367 - accuracy: 0.7251\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.7038 - accuracy: 0.7403\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.6601 - accuracy: 0.7586\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.6385 - accuracy: 0.7674\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.5991 - accuracy: 0.7829\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.5774 - accuracy: 0.7957\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.5558 - accuracy: 0.8032\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.5327 - accuracy: 0.8132\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.5107 - accuracy: 0.8351\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.4901 - accuracy: 0.8507\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.4682 - accuracy: 0.8612\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.4508 - accuracy: 0.8694\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.4383 - accuracy: 0.8735\n",
      "79/79 [==============================] - 1s 3ms/step - loss: 1.0184 - accuracy: 0.6292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0183591842651367, 0.6291999816894531]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InvPyramidNet.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1)\n",
    "InvPyramidNet.evaluate(x_test, y_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WEsw9Sbzy0gZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
